# ğŸ“š Projeto de ExtraÃ§Ã£o de Artigos sobre Desastres Naturais

Este projeto tem como objetivo **recolher, filtrar e enriquecer artigos de notÃ­cias relacionadas com desastres naturais** (como cheias, inundaÃ§Ãµes, tempestades) publicados em Portugal.

Usa como fonte principal o **Arquivo.pt**, extraindo notÃ­cias do **DiÃ¡rio de NotÃ­cias** e do **PÃºblico** entre os anos de **2020 e 2024**. O objetivo final Ã© gerar uma base de dados pronta para uso analÃ­tico: `disaster_db_ready.csv`.

---

## ğŸ§© Estrutura Geral do Projeto

```bash
news_scraper/
â”œâ”€â”€ config/                # ConfiguraÃ§Ãµes de keywords e municÃ­pios
â”œâ”€â”€ data/                  # Dados extraÃ­dos e processados
â”œâ”€â”€ processors/            # Scripts de limpeza e transformaÃ§Ã£o
â”œâ”€â”€ scrapers/              # Scrapers (ex: Arquivo.pt)
â”œâ”€â”€ utils/                 # FunÃ§Ãµes auxiliares
â”œâ”€â”€ main.py                # Ponto de entrada principal
â”œâ”€â”€ requirements.txt       # DependÃªncias do projeto
â””â”€â”€ README.md              # Este ficheiro
```

---

## ğŸ” Etapas do Processo

### 1. **Coleta de Dados (Arquivo.pt)**

- O ficheiro `scrapers/arquivo_pt.py`:
  - Acede Ã  API do Arquivo.pt.
  - Pesquisa notÃ­cias com combinaÃ§Ãµes `keyword + municÃ­pio`.
  - Extrai tÃ­tulo, texto principal, data, e metadados.

### 2. **Filtragem e ClassificaÃ§Ã£o**

- O ficheiro `processors/filtra_desastres.py`:
  - Aplica filtros semÃ¢nticos para garantir que os artigos sÃ£o relevantes.
  - Identifica tipo e subtipo de desastre.
  - Extrai nÃºmero de vÃ­timas, hora do evento e municÃ­pio mencionado.
  - Guarda o resultado em `artigos_filtrados.csv`.

### 3. **TransformaÃ§Ã£o Final**

- O ficheiro `processors/gera_db_ready.py`:
  - Limpa, normaliza e estrutura os dados para anÃ¡lise.
  - Produz o ficheiro `disaster_db_ready.csv` com os seguintes campos principais:

```csv
ID, type, subtype, date, year, month, day, hour,
georef, district, municipali, parish, DICOFREG,
source, sourcedate, sourcetype, page,
fatalities, injured, evacuated, displaced, missing,
DataScraping
```

---

## âš™ï¸ Requisitos

Instalar dependÃªncias com:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Como Executar

1. **Raspar artigos do Arquivo.pt**:

```bash
python scrapers/arquivo_pt.py
```

2. **Filtrar artigos relevantes sobre desastres**:

```bash
python processors/filtra_desastres.py
```

3. **Gerar dataset final para anÃ¡lise**:

```bash
python processors/gera_db_ready.py
```

---

## ğŸ“¦ Output Esperado

- `artigos_filtrados.csv`: artigos classificados como relevantes.
- `disaster_db_ready.csv`: dataset limpo e estruturado, pronto para anÃ¡lise.

---

## ğŸ§  Exemplos de Keywords (`config/keywords.json`)

```json
{
  "weather_terms": {
    "portuguese": [
      "cheias", "inundaÃ§Ã£o", "deslizamento", "tempestade", ...
    ]
  }
}
```

---

## ğŸ—ºï¸ GeorreferenciaÃ§Ã£o (`config/municipios.json`)

ContÃ©m todos os municÃ­pios portugueses para localizar geograficamente os eventos extraÃ­dos.

