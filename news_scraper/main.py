from scrapers import arquivo_pt
from processors.filtra_desastres import filtrar_e_transformar
from processors.gera_db_ready import gerar_disaster_db_ready

# ðŸ”§ Definir os domÃ­nios dos jornais
PUBLICO = "www.publico.pt"
DN = "www.DN.pt"
OUTPUT_CSV = "data/artigos_brutos.csv"


def run_all():
    print("\nðŸš€ Iniciando scraping de fontes de notÃ­cias (Arquivo.pt)...\n")
    all_articles = []

    for jornal in [PUBLICO, DN]:
        artigos = arquivo_pt.scrape(site=jornal)
        all_articles.extend(artigos)

    print(f"\nðŸ“Ž A guardar {len(all_articles)} artigos em: {OUTPUT_CSV}")
    arquivo_pt.exportar_csv(all_articles, OUTPUT_CSV)

    print("\nðŸ§¼ A iniciar filtro e transformaÃ§Ã£o...\n")
    filtrar_e_transformar()

    print("\nðŸ“Š A gerar ficheiro final com vÃ­timas...\n")
    gerar_disaster_db_ready()

    print("\nâœ… Pipeline completo.\n")


if __name__ == "__main__":
    run_all()