# SIMPREDE - Pipeline de Dados com Apache Airflow

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o do pipeline de dados SIMPREDE utilizando Apache Airflow em ambiente Docker. O sistema inclui web scraping do Google News, processamento de dados e exportaÃ§Ã£o para Google Cloud Storage.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#instalaÃ§Ã£o-e-configuraÃ§Ã£o)
- [ConfiguraÃ§Ã£o do Google Cloud Storage](#configuraÃ§Ã£o-do-google-cloud-storage)
- [ConfiguraÃ§Ã£o da Base de Dados](#configuraÃ§Ã£o-da-base-de-dados)
- [UtilizaÃ§Ã£o](#utilizaÃ§Ã£o)
- [DAGs DisponÃ­veis](#dags-disponÃ­veis)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [MonitorizaÃ§Ã£o e Logs](#monitorizaÃ§Ã£o-e-logs)
- [ResoluÃ§Ã£o de Problemas](#resoluÃ§Ã£o-de-problemas)
- [Comandos Ãšteis](#comandos-Ãºteis)
- [ContribuiÃ§Ã£o](#contribuiÃ§Ã£o)

## ğŸ¯ VisÃ£o Geral

O SIMPREDE Ã© um sistema de pipeline de dados automatizado que:

- **Web Scraping**: Extrai notÃ­cias do Google News relacionadas com eventos de desastres
- **Processamento**: Analisa e normaliza dados de notÃ­cias
- **GeocodificaÃ§Ã£o**: Identifica localizaÃ§Ãµes geogrÃ¡ficas nos artigos
- **ExportaÃ§Ã£o**: Guarda dados processados no Google Cloud Storage
- **Base de Dados**: Integra com PostgreSQL/Supabase para armazenamento persistente

### CaracterÃ­sticas Principais

- âœ… Interface web Apache Airflow para gestÃ£o de pipelines
- âœ… ExecuÃ§Ã£o automatizada via Docker
- âœ… Suporte para Google Cloud Storage
- âœ… Processamento de linguagem natural para portuguÃªs
- âœ… Sistema de geocodificaÃ§Ã£o para Portugal e MoÃ§ambique
- âœ… MonitorizaÃ§Ã£o e logs detalhados

## ğŸ›  PrÃ©-requisitos

### Software NecessÃ¡rio

- **Docker Desktop** (versÃ£o 4.0 ou superior)
- **Docker Compose** (incluÃ­do no Docker Desktop)
- **Git** para clonar o repositÃ³rio
- **Conta Google Cloud Platform** (opcional, para GCS)

### Requisitos do Sistema

- **macOS**: 10.15 ou superior
- **RAM**: MÃ­nimo 4GB, recomendado 8GB
- **EspaÃ§o em Disco**: MÃ­nimo 5GB livres
- **Portas**: 8080 (Airflow) e 5432 (PostgreSQL) disponÃ­veis

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clonar o RepositÃ³rio

```bash
git clone <url-do-repositorio>
cd simprede-airflow
```

### 2. ConfiguraÃ§Ã£o Inicial

Execute o script de arranque que configura automaticamente o ambiente:

```bash
cp .env.template .env
chmod +x start_airflow.sh
./start_airflow.sh
```

Este script irÃ¡:
- Verificar prÃ©-requisitos do Docker
- Configurar permissÃµes dos directÃ³rios
- Construir as imagens Docker
- Inicializar a base de dados Airflow
- Arrancar todos os serviÃ§os
- Apresentar as credenciais de administrador

### 3. Acesso Ã  Interface Web

ApÃ³s a inicializaÃ§Ã£o:
- **URL**: http://localhost:8080
- **Utilizador**: `admin`
- **Password**: *(gerada automaticamente e apresentada no terminal)*

## â˜ï¸ ConfiguraÃ§Ã£o do Google Cloud Storage

### OpÃ§Ã£o 1: Credenciais de Utilizador (Recomendado para Desenvolvimento)

```bash
# Instalar Google Cloud SDK
# Efectuar login
gcloud auth application-default login

# Definir projeto
gcloud config set project simprede-461309
```

### OpÃ§Ã£o 2: Service Account (Recomendado para ProduÃ§Ã£o)

1. **Criar Service Account no Google Cloud Console**:
   - VÃ¡ a https://console.cloud.google.com/
   - IAM & Admin â†’ Service Accounts
   - Criar nova Service Account
   - Atribuir role "Storage Admin"

2. **Baixar Credenciais**:
   - Baixar o ficheiro JSON de credenciais
   - Guardar como `./config/gcs-credentials.json`

3. **Reiniciar ServiÃ§os**:
   ```bash
   ./restart_airflow.sh
   ```

### Verificar ConfiguraÃ§Ã£o GCS

```bash
docker compose exec airflow-standalone python3 /opt/airflow/scripts/google_scraper/exportador_gcs/validate_gcs_setup.py
```

## ğŸ—„ ConfiguraÃ§Ã£o da Base de Dados

### Credenciais Supabase

O projeto estÃ¡ configurado para usar Supabase como base de dados PostgreSQL. As credenciais estÃ£o no ficheiro `.env`:

```bash
# Supabase/PostgreSQL Credenciais
DB_USER=postgres.kyrfsylobmsdjlrrpful
DB_PASSWORD=HXU3tLVVXRa1jtjo
DB_HOST=aws-0-eu-west-3.pooler.supabase.com
DB_PORT=6543
DB_NAME=postgres
DB_SCHEMA=google_scraper
```

### Testar LigaÃ§Ã£o Ã  Base de Dados

```bash
docker compose exec airflow-standalone python3 -c "
from airflow.providers.postgres.hooks.postgres import PostgresHook
hook = PostgresHook(postgres_conn_id='postgres_default')
print('âœ… LigaÃ§Ã£o PostgreSQL bem-sucedida')
"
```

## ğŸ“Š UtilizaÃ§Ã£o

### Arrancar o Sistema

```bash
./start_airflow.sh
```

### Parar o Sistema

```bash
./stop_airflow.sh
```

### Reiniciar o Sistema

```bash
./restart_airflow.sh
```

### Executar DAGs

1. Aceder Ã  interface web em http://localhost:8080
2. Na pÃ¡gina "DAGs", localizar o DAG desejado
3. Activar o DAG usando o toggle
4. Clicar em "Trigger DAG" para execuÃ§Ã£o manual

## ğŸ“ˆ DAGs DisponÃ­veis

### `daily_eventos_processing_dag`
- **DescriÃ§Ã£o**: Pipeline principal de processamento diÃ¡rio de eventos
- **FrequÃªncia**: DiÃ¡rio Ã s 06:00
- **Tarefas**:
  - Scraping de notÃ­cias do Google News
  - Processamento e anÃ¡lise de conteÃºdo
  - GeocodificaÃ§Ã£o de localizaÃ§Ãµes
  - ExportaÃ§Ã£o para GCS

### `pipeline_scraper_google`
- **DescriÃ§Ã£o**: Pipeline de web scraping especÃ­fico do Google News
- **FrequÃªncia**: Manual ou programado
- **Tarefas**:
  - ExtracÃ§Ã£o de notÃ­cias
  - NormalizaÃ§Ã£o de dados
  - Filtragem por relevÃ¢ncia

### `debug_coordinates_dag`
- **DescriÃ§Ã£o**: DAG de debug para geocodificaÃ§Ã£o
- **UtilizaÃ§Ã£o**: Testes e validaÃ§Ã£o de coordenadas

## ğŸ“ Estrutura do Projeto

```
simprede-airflow/
â”œâ”€â”€ Dockerfile                     # ConfiguraÃ§Ã£o Docker Airflow
â”œâ”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o de serviÃ§os
â”œâ”€â”€ requirements.txt               # DependÃªncias Python
â”œâ”€â”€ .env.template                # Exemplo de configuraÃ§Ã£o
â”œâ”€â”€ .env                          # VariÃ¡veis de ambiente
â”œâ”€â”€ README.md                     # Este ficheiro
â”œâ”€â”€ start_airflow.sh              # Script de arranque
â”œâ”€â”€ stop_airflow.sh               # Script de paragem
â”œâ”€â”€ restart_airflow.sh            # Script de reinÃ­cio
â”œâ”€â”€ config/                       # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ gcs-credentials.json      # Credenciais GCS (opcional)
â”‚   â””â”€â”€ gcs_env.sh               # VariÃ¡veis ambiente GCS
â”œâ”€â”€ dags/                         # DAGs Airflow
â”‚   â”œâ”€â”€ daily_eventos_processing_dag.py
â”‚   â”œâ”€â”€ pipeline_scraper_google.py
â”‚   â””â”€â”€ debug_coordinates_dag.py
â”œâ”€â”€ scripts/                      # Scripts de apoio
â”‚   â”œâ”€â”€ docker-entrypoint.sh     # Script de entrada Docker
â”‚   â”œâ”€â”€ init_gcs.sh              # InicializaÃ§Ã£o GCS
â”‚   â””â”€â”€ google_scraper/          # MÃ³dulos scraping
â”‚       â”œâ”€â”€ scraping/            # LÃ³gica de scraping
â”‚       â”œâ”€â”€ processador/         # Processamento dados
â”‚       â”œâ”€â”€ exportador_gcs/      # ExportaÃ§Ã£o GCS
â”‚       â”œâ”€â”€ extracao/           # ExtracÃ§Ã£o conteÃºdo
â”‚       â””â”€â”€ utils/              # UtilitÃ¡rios
â”œâ”€â”€ data/                        # Dados processados
â”‚   â”œâ”€â”€ raw/                    # Dados brutos
â”‚   â”œâ”€â”€ structured/             # Dados estruturados
â”‚   â””â”€â”€ processed/              # Dados processados
â”œâ”€â”€ logs/                       # Logs Airflow
â””â”€â”€ plugins/                    # Plugins Airflow
```

## ğŸ“Š MonitorizaÃ§Ã£o e Logs

### Interface Web Airflow

- **DAGs**: VisÃ£o geral de todos os pipelines
- **Task Instances**: Estado de execuÃ§Ã£o das tarefas
- **Logs**: Logs detalhados por tarefa
- **Gantt Chart**: VisualizaÃ§Ã£o temporal de execuÃ§Ã£o

### Logs do Sistema

```bash
# Logs do Airflow
docker compose logs airflow-standalone

# Logs da base de dados
docker compose logs postgres

# Logs em tempo real
docker compose logs -f airflow-standalone
```

### Logs de Dados

Os dados processados sÃ£o organizados por data:
```
data/
â”œâ”€â”€ raw/2025/06/05/           # Dados brutos
â”œâ”€â”€ structured/2025/06/05/    # Dados estruturados
â””â”€â”€ processed/2025/06/05/     # Dados processados
```

## ğŸ”§ ResoluÃ§Ã£o de Problemas

### Problema: Contentor nÃ£o arranca

```bash
# Verificar status dos contentores
docker compose ps

# Verificar logs para erros
docker compose logs airflow-standalone

# Reinicializar completamente
docker compose down -v
./start_airflow.sh
```

### Problema: Erro de permissÃµes

```bash
# Verificar e corrigir permissÃµes
sudo chown -R $(id -u):$(id -g) ./data ./logs ./config
chmod -R 755 ./data ./logs ./config
```

### Problema: Credenciais GCS

```bash
# Verificar configuraÃ§Ã£o
./scripts/debug_env.sh

# Validar credenciais
gcloud auth list
gcloud auth application-default login
```

### Problema: LigaÃ§Ã£o Ã  base de dados

```bash
# Testar ligaÃ§Ã£o
docker compose exec airflow-standalone python3 -c "
import psycopg2
conn = psycopg2.connect(
    host='aws-0-eu-west-3.pooler.supabase.com',
    port=6543,
    user='postgres.kyrfsylobmsdjlrrpful',
    password='HXU3tLVVXRa1jtjo',
    database='postgres'
)
print('âœ… LigaÃ§Ã£o PostgreSQL bem-sucedida')
"
```

## ğŸ’¡ Comandos Ãšteis

### GestÃ£o de Contentores

```bash
# Ver status dos serviÃ§os
docker compose ps

# Aceder ao shell do contentor Airflow
docker compose exec airflow-standalone bash

# Reconstruir imagens
docker compose build --no-cache

# Limpar tudo (cuidado: remove dados)
docker compose down -v
docker system prune -a
```

### Testes e ValidaÃ§Ã£o

```bash
# Testar providers instalados
chmod +x scripts/test_providers.sh
./scripts/test_providers.sh

# Validar configuraÃ§Ã£o GCS
docker compose exec airflow-standalone python3 /opt/airflow/scripts/google_scraper/exportador_gcs/validate_gcs_setup.py

# Debug de ambiente
./scripts/debug_env.sh
```

### GestÃ£o de Dados

```bash
# Limpar dados antigos (manter Ãºltimos 7 dias)
find ./data -type f -mtime +7 -delete

# Fazer backup dos dados
tar -czf backup_$(date +%Y%m%d).tar.gz ./data ./config

# Ver tamanho dos dados
du -sh ./data/*
```

## ğŸ”„ Fluxo de Trabalho TÃ­pico

1. **Arrancar o sistema**: `./start_airflow.sh`
2. **Aceder Ã  interface**: http://localhost:8080
3. **Activar DAGs**: Toggle nos DAGs desejados
4. **Monitorizar execuÃ§Ã£o**: Ver logs e estado das tarefas
5. **Verificar dados**: Consultar ficheiros em `./data/`
6. **Verificar GCS**: Confirmar upload no bucket GCS

## ğŸ¤ ContribuiÃ§Ã£o

### Desenvolvimento Local

1. Fazer fork do repositÃ³rio
2. Criar branch para a funcionalidade: `git checkout -b feature/nova-funcionalidade`
3. Fazer alteraÃ§Ãµes e testar localmente
4. Fazer commit: `git commit -m "DescriÃ§Ã£o da alteraÃ§Ã£o"`
5. Push para o branch: `git push origin feature/nova-funcionalidade`
6. Criar Pull Request

### PadrÃµes de CÃ³digo

- **Python**: Seguir PEP 8
- **DAGs**: Documentar todas as tarefas
- **Commits**: Mensagens descritivas em portuguÃªs
- **Logs**: Usar logging adequado com nÃ­veis apropriados

## ğŸ“ Suporte

Para questÃµes e suporte:

1. **Logs**: Consultar primeiro os logs do sistema
2. **DocumentaÃ§Ã£o**: Verificar este README
3. **Issues**: Criar issue no repositÃ³rio com:
   - DescriÃ§Ã£o do problema
   - Logs relevantes
   - Passos para reproduzir
   - Ambiente (OS, Docker version, etc.)

---

**SIMPREDE** - Sistema Integrado de MonitorizaÃ§Ã£o e PrevenÃ§Ã£o de Desastres  

